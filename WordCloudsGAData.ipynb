{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "This project analyzes article text based on 180day unique views obtained from Google Analytics with views set at the 180day goal and no views not meeting that goal. Word clouds and bag of words used to analyze text. The data file should contain views data captured from Google Analytics as well as scraped article text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import brier_score_loss\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting for unique views at 180 days\n",
    "Use Google Anaytics to obtain unique views at 180 days and calculate binary\n",
    "0 = no view\n",
    "1 = view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"File.csv\")\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_df = df.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = [col for col in df.columns if (df[col].dtype=='int64' or df[col].dtype=='float64') and col != 'Exited']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical_columns].describe().loc[['min','max', 'mean','50%'],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['iContact_messageId'] == df['iContact_messageId'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = df.drop(columns=['Month'],axis=1)\n",
    "label = df[\"Month\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##clean textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "stopset = list(set(stopwords.words('english')))\n",
    "clean_reviews_text = []\n",
    "for review in df['ArticleText']:  # Loop through the tokens (the words or symbols) in each review. \n",
    "    try:    \n",
    "        cleaned_review = re.sub(\"[^a-zA-Z]\",\" \", review)  # Remove numbers and punctuation.\n",
    "        cleaned_review = cleaned_review.lower()  # Convert the text to lower case.\n",
    "        cleaned_review = ' '.join([word for word in cleaned_review.split() if word not in stopset])  # Keep only words that are not stopwords.\n",
    "        cleaned_review = ' '.join([wordnet_lemmatizer.lemmatize(word, pos='n') for word in cleaned_review.split()])  # Keep each noun's lemma.\n",
    "        cleaned_review = ' '.join([wordnet_lemmatizer.lemmatize(word, pos='v') for word in cleaned_review.split()])  # Keep each verb's lemma.\n",
    "        cleaned_review = re.sub(r\"(http\\S+)\",\" \", cleaned_review)  # Remove http links.\n",
    "        cleaned_review = ' '.join(cleaned_review.split())  # Remove white space.\n",
    "    except TypeError:\n",
    "        pass\n",
    "    clean_reviews_text.append(cleaned_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##show dataframe with cleanText column added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleanText'] = clean_reviews_text\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_long_string =  ' '.join(df['cleanText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##plot word cloud of all texts\n",
    "wordcloud = WordCloud(background_color='silver',max_font_size = 50,random_state=75, relative_scaling=.3, collocations=True).generate(one_long_string)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.title('All texts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create dataframe of unique views and no unique views, set views goal based on company unique view goals\n",
    "df_noviews = df[df['uniqueviews'] == 0]\n",
    "df_views = df[df['uniqueviews'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_long_string_noviews =  ' '.join(df_noviews['cleanText'])\n",
    "one_long_string_views =  ' '.join(df_views['cleanText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud_noviews = WordCloud(background_color='silver',max_font_size = 50,random_state=201).generate(one_long_string_noviews)\n",
    "wordcloud_views = WordCloud(background_color='silver',max_font_size = 50,random_state=201).generate(one_long_string_views)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "plt.subplot(121)  # 121 means 1 row and 2 columns of plots and this is the first subplot.\n",
    "plt.imshow(wordcloud_noviews)\n",
    "plt.axis(\"off\")\n",
    "plt.title('Unique Views at 180 Days 750 or Below') \n",
    "\n",
    "plt.subplot(122)  # 122 means 1 row and 2 columns of plots and this is the second subplot.\n",
    "plt.imshow(wordcloud_views)\n",
    "plt.axis(\"off\")\n",
    "plt.title('Unique Views at 180 Days above 750')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##show bag of words dataframe with word usage\n",
    "corpus = list(df['cleanText'])\n",
    "corpus_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=30)\n",
    "bag_of_words = corpus_vectorizer.fit_transform(corpus)\n",
    "bag_of_words_df = pd.DataFrame(bag_of_words.toarray(), columns=corpus_vectorizer.get_feature_names())\n",
    "bag_of_words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(bag_of_words_df.mean(axis=0), index=bag_of_words_df.columns, columns=['Avg count (all texts)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_noviews = list(df_noviews['cleanText'])\n",
    "corpus_noviews_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=20)\n",
    "bag_of_words_noviews = corpus_noviews_vectorizer.fit_transform(corpus_noviews)\n",
    "bag_of_words_noviews_df = pd.DataFrame(bag_of_words_noviews.toarray(), columns=corpus_noviews_vectorizer.get_feature_names())\n",
    "bag_of_words_noviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_views = list(df_views['cleanText'])\n",
    "corpus_views_vectorizer = CountVectorizer(ngram_range=(1, 2), max_features=20)\n",
    "bag_of_words_views = corpus_views_vectorizer.fit_transform(corpus_views)\n",
    "bag_of_words_views_df = pd.DataFrame(bag_of_words_views.toarray(), columns=corpus_views_vectorizer.get_feature_names())\n",
    "bag_of_words_views_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words_noviews = pd.DataFrame(bag_of_words_noviews_df.mean(axis=0), \n",
    "                                     index=bag_of_words_noviews_df.columns, columns=['Avg count (noviews)'])\n",
    "freq_words_views = pd.DataFrame(bag_of_words_views_df.mean(axis=0), \n",
    "                                     index=bag_of_words_views_df.columns, columns=['Avg count (views)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_words_noviews.join(freq_words_views, how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load a happiness dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.request\n",
    "url='http://hedonometer.org/api/v1/words/?format=json'\n",
    "data = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "loaded_json = json.loads(data)\n",
    "happ_dict = loaded_json['objects']\n",
    "from pandas.io.json import json_normalize\n",
    "happ_df = json_normalize(happ_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_happs_df = happ_df[['word', 'happs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_happs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##load the Harvard General Inquirer Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genInq_df = pd.read_csv('inquirerbasic.csv', encoding='latin-1', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genInq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genInq_df['Entry'] = [word.lower() for word in genInq_df['Entry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.concat([pd.DataFrame(genInq_df.columns), pd.DataFrame(genInq_df.dtypes.values.reshape(-1,1)),\n",
    "                        pd.DataFrame(genInq_df.isnull().sum().values), pd.DataFrame([genInq_df[name].nunique() for name in genInq_df.columns])],\n",
    "                       axis=1)\n",
    "summary_df.columns = ['Variable Name', 'Data Type', 'Nulls', 'Unique Values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##do some feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)  # First reset the index of the data frame. Drops first column so that don't interfere with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wildfire_dummy = []\n",
    "for i in range(0, len(df)):\n",
    "    text = df.loc[i]['cleanText']\n",
    "    wildfire_dummy.append(int('go' in set(text.split())))\n",
    "df['contains_wildfire'] = wildfire_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "happs_list = []\n",
    "for i in range(0, len(df)):\n",
    "    text = df.loc[i]['cleanText']\n",
    "    text_df = pd.DataFrame(pd.Series(text.split()), columns=['word'])\n",
    "    text_happs_df = pd.merge(text_df, word_happs_df, on='word')\n",
    "    happs_list.append(text_happs_df['happs'].sum())\n",
    "df['happ_score'] = happs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postivie_dummy_df = pd.get_dummies(genInq_df[['Entry','Positiv']], prefix=['contains'], columns=['Positiv'])\n",
    "postivie_dummy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "positive_list = []\n",
    "for i in range(0, len(df)):\n",
    "    text = df.loc[i]['cleanText']\n",
    "    text_df = pd.DataFrame(pd.Series(text.split()), columns=['Entry'])\n",
    "    text_positive_df = pd.merge(text_df, postivie_dummy_df, on='Entry')\n",
    "    positive_list.append(text_positive_df['contains_Positiv'].sum())\n",
    "df['positive_score'] = positive_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "risk_dummy = []\n",
    "for i in range(0, len(df)):\n",
    "    text = df.loc[i]['cleanText']\n",
    "    risk_dummy.append(int('risk' in set(text.split())))\n",
    "df['contains_risk'] = risk_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "happs_list = []\n",
    "for i in range(0, len(df)):\n",
    "    text = df.loc[i]['cleanText']\n",
    "    text_df = pd.DataFrame(pd.Series(text.split()), columns=['word'])\n",
    "    text_happs_df = pd.merge(text_df, word_happs_df, on='word')\n",
    "    happs_list.append(text_happs_df['happs'].sum())\n",
    "df['happ_score'] = happs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postivie_dummy_df = pd.get_dummies(genInq_df[['Entry','Positiv']], prefix=['contains'], columns=['Positiv'])\n",
    "postivie_dummy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "positive_list = []\n",
    "for i in range(0, len(df)):\n",
    "    text = df.loc[i]['cleanText']\n",
    "    text_df = pd.DataFrame(pd.Series(text.split()), columns=['Entry'])\n",
    "    text_positive_df = pd.merge(text_df, postivie_dummy_df, on='Entry')\n",
    "    positive_list.append(text_positive_df['contains_Positiv'].sum())\n",
    "df['positive_score'] = positive_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "land_dummy = []\n",
    "for i in range(0, len(df)):\n",
    "    text = df.loc[i]['cleanText']\n",
    "    land_dummy.append(int('risk' in set(text.split())))\n",
    "df['contains_land'] = land_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "happs_list = []\n",
    "for i in range(0, len(df)):\n",
    "    text = df.loc[i]['cleanText']\n",
    "    text_df = pd.DataFrame(pd.Series(text.split()), columns=['word'])\n",
    "    text_happs_df = pd.merge(text_df, word_happs_df, on='word')\n",
    "    happs_list.append(text_happs_df['happs'].sum())\n",
    "df['happ_score'] = happs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postivie_dummy_df = pd.get_dummies(genInq_df[['Entry','Positiv']], prefix=['contains'], columns=['Positiv'])\n",
    "postivie_dummy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "positive_list = []\n",
    "for i in range(0, len(df)):\n",
    "    text = df.loc[i]['cleanText']\n",
    "    text_df = pd.DataFrame(pd.Series(text.split()), columns=['Entry'])\n",
    "    text_positive_df = pd.merge(text_df, postivie_dummy_df, on='Entry')\n",
    "    positive_list.append(text_positive_df['contains_Positiv'].sum())\n",
    "df['positive_score'] = positive_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "economic_dummy = []\n",
    "for i in range(0, len(df)):\n",
    "    text = df.loc[i]['cleanText']\n",
    "    economic_dummy.append(int('economic' in set(text.split())))\n",
    "df['contains_economic'] = economic_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "happs_list = []\n",
    "for i in range(0, len(df)):\n",
    "    text = df.loc[i]['cleanText']\n",
    "    text_df = pd.DataFrame(pd.Series(text.split()), columns=['word'])\n",
    "    text_happs_df = pd.merge(text_df, word_happs_df, on='word')\n",
    "    happs_list.append(text_happs_df['happs'].sum())\n",
    "df['happ_score'] = happs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postivie_dummy_df = pd.get_dummies(genInq_df[['Entry','Positiv']], prefix=['contains'], columns=['Positiv'])\n",
    "postivie_dummy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "positive_list = []\n",
    "for i in range(0, len(df)):\n",
    "    text = df.loc[i]['cleanText']\n",
    "    text_df = pd.DataFrame(pd.Series(text.split()), columns=['Entry'])\n",
    "    text_positive_df = pd.merge(text_df, postivie_dummy_df, on='Entry')\n",
    "    positive_list.append(text_positive_df['contains_Positiv'].sum())\n",
    "df['positive_score'] = positive_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##split into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_variables_selected = ['contains_economic','happ_score', 'positive_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig_train, X_test, y_orig_train, y_test = train_test_split(df[ind_variables_selected], df['uniqueviews'], test_size=0.25, random_state=201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_orig_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_orig_train, y_orig_train, test_size=0.2, random_state=201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##set up scoring rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_score(predictions, realizations):\n",
    "    this_brier_score = 1 - brier_score_loss(realizations, predictions)\n",
    "    return this_brier_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skill_score(predictions, realizations):\n",
    "    naive = np.repeat(np.mean(y_train), len(realizations))\n",
    "    this_skill_score = (brier_score(predictions, realizations) - brier_score(naive, realizations)) / (1 - brier_score(naive, realizations))\n",
    "    return this_skill_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##fit a classification tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(min_samples_split=5000, max_depth=50, random_state=201)\n",
    "dt_model = dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(dt_model.feature_importances_, index=ind_variables_selected).sort_values(by = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_test_prob_all = pd.DataFrame(dt_model.predict_proba(X_valid))\n",
    "dt_test_prob = dt_test_prob_all[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skill_score(dt_test_prob, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = dt.fit(X_orig_train, y_orig_train)\n",
    "dt_test_prob_all = pd.DataFrame(dt_model.predict_proba(X_test))\n",
    "dt_test_prob = dt_test_prob_all[1]\n",
    "skill_score(dt_test_prob, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_classifier = SVC(kernel='rbf')\n",
    "support_vector_classifier.fit(X_train,y_train)\n",
    "y_pred_svc = support_vector_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm_support_vector_classifier = confusion_matrix(y_test,y_pred_svc)\n",
    "print(cm_support_vector_classifier,end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator = cm_support_vector_classifier[0][0] + cm_support_vector_classifier[1][1]\n",
    "denominator = sum(cm_support_vector_classifier[0]) + sum(cm_support_vector_classifier[1])\n",
    "acc_svc = (numerator/denominator) * 100\n",
    "print(\"Accuracy : \",round(acc_svc,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_svc = cross_val_score(estimator = SVC(kernel = 'rbf'), X = X_train, y = y_train, cv = 10, n_jobs = -1)\n",
    "print(\"Cross Validation Accuracy : \",round(cross_val_svc.mean() * 100 , 2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier = RandomForestClassifier()\n",
    "random_forest_classifier.fit(X_train,y_train)\n",
    "y_pred_rfc = random_forest_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_random_forest_classifier = confusion_matrix(y_test,y_pred_rfc)\n",
    "print(cm_random_forest_classifier,end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator = cm_random_forest_classifier[0][0] + cm_random_forest_classifier[1][1]\n",
    "denominator = sum(cm_random_forest_classifier[0]) + sum(cm_random_forest_classifier[1])\n",
    "acc_rfc = (numerator/denominator) * 100\n",
    "print(\"Accuracy : \",round(acc_rfc,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_rfc = cross_val_score(estimator=RandomForestClassifier(), X=X_train, y=y_train, cv=10, n_jobs=-1)\n",
    "print(\"Cross Validation Accuracy : \",round(cross_val_rfc.mean() * 100 , 2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_classifier = XGBClassifier()\n",
    "xgb_classifier.fit(X_train,y_train)\n",
    "y_pred_xgb = xgb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_xgb_classifier = confusion_matrix(y_test,y_pred_xgb)\n",
    "print(cm_xgb_classifier,end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerator = cm_xgb_classifier[0][0] + cm_xgb_classifier[1][1]\n",
    "denominator = sum(cm_xgb_classifier[0]) + sum(cm_xgb_classifier[1])\n",
    "acc_xgb = (numerator/denominator) * 100\n",
    "print(\"Accuracy : \",round(acc_xgb,2),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_xgb = cross_val_score(estimator=XGBClassifier(), X=X_train, y=y_train, cv=10, n_jobs=-1)\n",
    "print(\"Cross Validation Accuracy : \",round(cross_val_xgb.mean() * 100 , 2),\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
